{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Parsing HTML (Web scraping)",
      "provenance": [],
      "collapsed_sections": [
        "hhSDA6kvyzU_"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bk-oAQ5Xym3A"
      },
      "source": [
        "# Module 4 Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhSDA6kvyzU_"
      },
      "source": [
        "\n",
        "###  Question 1.\n",
        "The following Python script scrapes payroll data from the following website: http://kanview.ks.gov/\n",
        "\n",
        "\"KanView is an online solution that brings better visibility, openness, and accountability to Kansas State Government. KanView contains data related to Government spending and income, allowing Kansas taxpayers an inside look at Government financial activity.\"\n",
        "\n",
        "The script scrapes the payroll data for the Fort Hays State University (https://www.fhsu.edu/) using Selenium drive along with Beautiful soup parsing library. It stores the scraped results in a Pandas DataFrame. It converts the DataFrame into a corresponding JSON object. It finally writes the JSON object into a json file in your local file system.\n",
        "\n",
        "Modify this script to scrape the wages data for the University of Kansas (https://ku.edu/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TE2f9WoZmsv5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15badee7-02b9-433c-981c-c0a5a9545bad"
      },
      "source": [
        "%%bash\n",
        "chmod 777 /tmp\n",
        "mkdir data\n",
        "apt-get update --allow-unauthenticated \n",
        "apt-get update -y --fix-missing \n",
        "pip install selenium\n",
        "apt-get install chromium-chromedriver -y --fix-missing\n",
        "pip install joblib\n",
        "apt-get update --fix-missing"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ign:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:4 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:8 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:14 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists...\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:13 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:14 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists...\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.7/dist-packages (3.141.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from selenium) (1.24.3)\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "chromium-chromedriver is already the newest version (91.0.4472.77-0ubuntu0.18.04.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 97 not upgraded.\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Ign:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:12 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:14 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Agzho5tfpXHT"
      },
      "source": [
        "## Headless Browser with BeautifulSoup\n",
        "import selenium\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "\n",
        "import re                      # for handling regular expressions\n",
        "import pandas as pd            # using pandas library\n",
        "from tabulate import tabulate  # tabulate, which takes a list of lists or another tabular data type as the first argument, and outputs a nicely formatted plain-text table\n",
        "\n",
        "\n",
        "options = Options()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "options.binary_location = '/usr/bin/chromium-browser'\n",
        "\n",
        "driver = webdriver.Chrome(executable_path='/usr/bin/chromedriver', options = options)\n",
        "driver.get(\"http://kanview.ks.gov/PayRates/PayRates_Agency.aspx\")\n",
        "\n",
        "\n",
        "#After opening the url above, Selenium clicks the specific agency link, in this case FHSU (Fort Hays State University)\n",
        "python_button = driver.find_element_by_id('MainContent_uxLevel1_Agencies_uxAgencyBtn_86') #FHSU\n",
        "python_button.click() #click fhsu link\n",
        "\n",
        "\n",
        "#Selenium hands the page source to Beautiful Soup\n",
        "soup_level1=BeautifulSoup(driver.page_source, 'lxml')\n",
        "\n",
        "\n",
        "datalist = [] #empty list\n",
        "x = 0 #counter\n",
        "\n",
        "#Beautiful Soup finds all Job Title links on the agency page and the loop begins\n",
        "for link in soup_level1.find_all('a', id=re.compile(\"^MainContent_uxLevel2_JobTitles_uxJobTitleBtn_\")):\n",
        "    \n",
        "    #Selenium visits each Job Title page\n",
        "    python_button = driver.find_element_by_id('MainContent_uxLevel2_JobTitles_uxJobTitleBtn_' + str(x))\n",
        "    python_button.click() #click link\n",
        "    \n",
        "    #Selenium hands of the source of the specific job page to Beautiful Soup\n",
        "    soup_level2=BeautifulSoup(driver.page_source, 'lxml')\n",
        "\n",
        "    #Beautiful Soup grabs the HTML table on the page\n",
        "    table = soup_level2.find_all('table')[0]\n",
        "    \n",
        "    #Giving the HTML table to pandas to put in a dataframe object\n",
        "    df = pd.read_html(str(table),header=0)\n",
        "    \n",
        "    #Store the dataframe in a list\n",
        "    datalist.append(df[0])\n",
        "    \n",
        "    #Ask Selenium to click the back button\n",
        "    driver.execute_script(\"window.history.go(-1)\") \n",
        "    \n",
        "    #increment the counter variable before starting the loop over\n",
        "    x += 1\n",
        "    \n",
        "    #end loop block\n",
        "    \n",
        "#loop has completed\n",
        "\n",
        "#end the Selenium browser session\n",
        "driver.quit()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3CJn8uaqY-s"
      },
      "source": [
        "#combine all pandas dataframes in the list into one big dataframe\n",
        "result = pd.concat([pd.DataFrame(datalist[i]) for i in range(len(datalist))],ignore_index=True)\n",
        "\n",
        "#convert the pandas dataframe to JSON\n",
        "json_records = result.to_json(orient='records')\n",
        "\n",
        "#pretty print to CLI with tabulate\n",
        "#converts to an ascii table\n",
        "print(tabulate(result, headers=[\"Employee Name\",\"Job Title\",\"Overtime Pay\",\"Total Gross Pay\"],tablefmt='psql'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNjbvEKEtFsl"
      },
      "source": [
        "#open, writem and download the file to your local file system\n",
        "from google.colab import files\n",
        "\n",
        "with open('payroll_data.json', 'w') as f:\n",
        "  f.write(json_records)\n",
        "\n",
        "files.download('payroll_data.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcjDkXac6DG6"
      },
      "source": [
        "### Question 2.\n",
        "\n",
        "Using .decribe() method in Pandas (https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html), generate descriptive statistics for Total Gross Pay and Overtime Pay.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqkyk0aa6qf9"
      },
      "source": [
        "# Answer:\n",
        "\n",
        "# you may find the following stackoverflow info helpful\n",
        "# https://stackoverflow.com/questions/32464280/converting-currency-with-to-numbers-in-python-pandas\n",
        "result['Total Gross Pay'].str.replace('$', '').str.replace(',', '').astype(float).mean()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjHtW23a_-tu"
      },
      "source": [
        "result['Overtime Pay'].str.replace('$', '').str.replace(',', '').astype(float).describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omTzoZX97xg_"
      },
      "source": [
        "### Question 3.\n",
        "Using Pandas, calculate the average Total Gross Pay for Education Program Coordinator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae7KIecp8J09"
      },
      "source": [
        "# Answer:\n",
        "\n",
        "epc= result.loc[result['Job Title']== 'Education Program Coordinator']\n",
        "\n",
        "print('The average total gross pay for an education program coordinator is')\n",
        "epc['Total Gross Pay'].str.replace('$', '').str.replace(',', '').astype(float).mean()     \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}